{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KgriPT0wETsq",
        "KqojleITLGJS",
        "HoTBod--M9Ci",
        "uvWJe_YqK_kJ",
        "2bJw45b2BWqx",
        "eBnbCu6GHbMJ",
        "ovVn780GHrrc",
        "QiC7Z0ACNWjd",
        "7mFVDgeZOzrU",
        "R96V-csIdC1p",
        "HONDBNqJsIqU",
        "4wvxrND85t7r",
        "VoOMHhKU8kFo",
        "alCnJ2ZtTNJt",
        "1BEvzpOwa-RD",
        "8N7jK4SvFZEb",
        "vaf9z1oLXgjx",
        "XjQz4ZoHhm1v",
        "xhfYzD5Oz7cg",
        "woJ2_wR5kc7h",
        "DboKBdVg48mc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgriPT0wETsq"
      },
      "source": [
        "### mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "lzcb1Hu3C9Sh",
        "outputId": "4c418da9-7f12-4a8b-b45d-b9a63727cefb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%pwd\n",
        "\n",
        "\"\"\"\n",
        "Use this javascript code in inspect>console so you wont need to click the page every 15 min:\n",
        "\n",
        "########################\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "########################\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqojleITLGJS"
      },
      "source": [
        "### change current path to where the working project folder is at"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmCJ8OpxKDuf",
        "outputId": "85d85b76-cadf-4d76-9d32-9151f6095589"
      },
      "source": [
        "%cd drive/MyDrive/projects/transformers_translation/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/projects/transformers_translation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoTBod--M9Ci"
      },
      "source": [
        "# Step 0: Get The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvWJe_YqK_kJ"
      },
      "source": [
        "### upload the data to our current path and unzip it (uncomment and run this only once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxOPocG6D3M2"
      },
      "source": [
        "# # data is from: https://www.statmt.org/europarl/ you can use this or just upload your own data\n",
        "# %cd data\n",
        "# !wget https://www.statmt.org/europarl/v7/de-en.tgz\n",
        "# !tar -xvf de-en.tgz\n",
        "# %cd ..\n",
        "# %pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bJw45b2BWqx"
      },
      "source": [
        "### Get non breaking prefixes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI3vtoy1BXOh"
      },
      "source": [
        "# get non_breaking_prefixes from https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes\n",
        "# then rename them to: \"nonbreaking_prefix.en\" and \"nonbreaking_prefix.de\" and put them in your data folder so we dont consider the\n",
        "# dot in 'mr.jackson' as the end of a sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBnbCu6GHbMJ"
      },
      "source": [
        "# Step 1: Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjF371EVHM6n"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time # to see how long it takes in training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwAD6AfAJSWp"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds # tools for the tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovVn780GHrrc"
      },
      "source": [
        "# Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiC7Z0ACNWjd"
      },
      "source": [
        "### read files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My08LADJHrFo",
        "outputId": "2d453144-dcfb-4527-e3ab-d7205cf6c4b3"
      },
      "source": [
        "with open(\"data/europarl-v7.de-en.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_en = f.read()\n",
        "\n",
        "with open(\"data/europarl-v7.de-en.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_de = f.read()\n",
        "\n",
        "print(text_en[:50])\n",
        "print(text_de[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resumption of the session\n",
            "I declare resumed the se\n",
            "Wiederaufnahme der Sitzungsperiode\n",
            "Ich erkl√§re die\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqSKydTgW4nE",
        "outputId": "63abdee8-48fa-4352-cbc7-1c33f884ed2d"
      },
      "source": [
        "\n",
        "with open(\"data/nonbreaking_prefix.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "\n",
        "with open(\"data/nonbreaking_prefix.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_de = f.read()\n",
        "print(non_breaking_prefix_en[:5])\n",
        "print(non_breaking_prefix_de[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "B\n",
            "C\n",
            "A\n",
            "B\n",
            "C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mFVDgeZOzrU"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Skngci4mOOiO",
        "outputId": "3a061492-fe95-4fb8-b653-a3abe8f609ca"
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref.lower() + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_de = non_breaking_prefix_de.split(\"\\n\")\n",
        "non_breaking_prefix_de = [' ' + pref.lower() + '.' for pref in non_breaking_prefix_de]\n",
        "print(non_breaking_prefix_en[:5])\n",
        "print(non_breaking_prefix_de[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' a.', ' b.', ' c.', ' d.', ' e.']\n",
            "[' a.', ' b.', ' c.', ' d.', ' e.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgcNPbA8SE9b"
      },
      "source": [
        "for prefix in non_breaking_prefix_en:\n",
        "    text_en = text_en.replace(prefix, prefix + '###')\n",
        "text_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_en)\n",
        "text_en = re.sub(r\"\\.###\", '', text_en)\n",
        "text_en = re.sub(r\" +\", ' ', text_en)\n",
        "text_en = text_en.replace('###', ' ')\n",
        "\n",
        "text_en = text_en.split(\"\\n\")\n",
        "\n",
        "for prefix in non_breaking_prefix_de:\n",
        "    text_de = text_de.replace(prefix, prefix + '###')\n",
        "text_de = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_de)\n",
        "text_de = re.sub(r\"\\.###\", '', text_de)\n",
        "text_de = re.sub(r\" +\", ' ', text_de)\n",
        "text_de = text_de.replace('###', ' ')\n",
        "\n",
        "text_de = text_de.split(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R96V-csIdC1p"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzNjGhLKT8OR"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_en, target_vocab_size=8000)\n",
        "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_de, target_vocab_size=8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z5zOdOEgvVm"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_DE = tokenizer_de.vocab_size + 2\n",
        "\n",
        "# we put start and end tokens as size-1 and size-2 which are the same as\n",
        "# tokenizer_size and tokenizer_size+1 because the words are from [0 to ts-1]\n",
        "# tokenize_en.encode(sentence) give a list then list + list + list appends them\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in text_en]\n",
        "outputs = [[VOCAB_SIZE_DE-2] + tokenizer_de.encode(sentence) + [VOCAB_SIZE_DE-1]\n",
        "          for sentence in text_de]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONDBNqJsIqU"
      },
      "source": [
        "### Remove too long sentences\n",
        "Why?\n",
        "(1) because when we pad we will have a hugeeee ram issuie for example sentence sizes of 1,100,2 when we pad they become 100,100,100 which we would rather loose that 100 than pad all to 100\n",
        "(2) takes too much time to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRoMWeU7r-tD"
      },
      "source": [
        "MAX_LENGTH = 20 # we will still have a lot of data with max len of 20\n",
        "\n",
        "# this part, why we do it is a bit tricky, pay attention why we do it like this:\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "# we remove in reversed because of shifting issuies when we start from begining\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# same stuff for outputs>20\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wvxrND85t7r"
      },
      "source": [
        "### input/output creation\n",
        "1) padding  \n",
        "2) batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47hjlJ1SvXwK"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVq4wN816gOj"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 # how much data to keep\n",
        "\n",
        "# now we turned our data into a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "# this is something that improves the way the dataset is stored, it increases\n",
        "# the speed of accessing the data which increases training speed in return:\n",
        "dataset = dataset.cache()\n",
        "\n",
        "# now we shuffle in batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# this increases the speed even further:\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoOMHhKU8kFo"
      },
      "source": [
        "# Step 3: Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alCnJ2ZtTNJt"
      },
      "source": [
        "## A - Positional Encoding (look at the formula in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoCaYr408as-"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        # this Positional Encoder we made it a child of the Layers so it has all\n",
        "        # the properties that a layer has\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        \"\"\"\n",
        "        :pos: (seq_len, 1) index of the word in sentence [0 to 19]\n",
        "        :i: the dimensions of the embedding (glove dims 200) then-> [0 to 199]\n",
        "        :d_model: the size (dimension) of the embeded (e.g. glove size 200)\n",
        "        :return: (seq_len, d_model) why? we are getting the encoding of the\n",
        "                every positions vs every one of the dimensions of that word\n",
        "        \"\"\"\n",
        "        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # dim: (seq_len, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input.shape = [batch_size, multihead_size(sz=8), each word (pos), that words embedding]\n",
        "        # keep in mind we DONT change the values of the input considering\n",
        "        # their positions, we just get the dims from input and calculate\n",
        "        # pos encoding totally seperatly and stack them at the end\n",
        "        seq_length = inputs.shape.as_list()[-2] # basically the pos\n",
        "        d_model = inputs.shape.as_list()[-1] # basically the embedded values\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # we do this because it has a [batch] dimension at the begining we add\n",
        "        # it. why? because inputs and the encodings need to be same dims so we\n",
        "        # make newaxis which it doesnt put 0's.... it copies those same dims for\n",
        "        # all the batches...\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        # now we need to return both the inputs and their pos_encodings\n",
        "        # but we have pos_encoding in np so we make them tf\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BEvzpOwa-RD"
      },
      "source": [
        "## B - Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8AbGEHbFua"
      },
      "source": [
        "### Attention computation (see the formula in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4SX3F__VjXc"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Q*K will be [output_len, d_model] * [d_model, input_len] which both are 20\n",
        "    # for both english and french\n",
        "    # the transpose_b=True makes keys turn to keys.T\n",
        "    # each of them are this dim: [batch_size, nb_proj, seq_len, d_proj]\n",
        "    # so with transpose it become: [a,b,c,d] * []\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32) # makes the dim_num float\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim) # scales it (formula stuff)\n",
        "\n",
        "    # because this mask as the paper said is optional to prevent the program\n",
        "    # from seeing the feauture. why? because when we backprop then they will\n",
        "    # consider the stuff in front of them so to stop this we add -1e9 to them\n",
        "    # so after softmax the probabilities become 0 for them\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "\n",
        "    # we apply the softmax along the last axis because we want their sum to be 1\n",
        "    # scaled_product = [output_len, input_len] -> softmax on input_len so\n",
        "    # basically we are keeping in_len the same but finding the probs for out_len\n",
        "    # so for every ins what is the prob of each of the outs\n",
        "    # (e.g. ith input, the probs [0.3,0.7] of the outs)\n",
        "    probs = tf.nn.softmax(scaled_product, axis=-1)\n",
        "\n",
        "    # attention = [output_len, input_len] * [input_len, d_model] = [output_len, d_model]\n",
        "    # so now we have d_model weights for each of the output words which we will\n",
        "    # feed to forwards to see their prediction for each of the out_lens\n",
        "    attention = tf.matmul(probs, values)\n",
        "\n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7xS01sz8Yzn"
      },
      "source": [
        "# import numpy as np\n",
        "# # this is just a test for you to see what happens in this line of code to the\n",
        "# # dims (which we realized from the 4 dims, it only transposed the last two and\n",
        "# # did mult only on those last two because matmul considers the other dims as\n",
        "# # batch size and other stuff) (tf.matmul(a, b, transpose_b=True))\n",
        "# a = np.arange(24).reshape(1,2,3,4)\n",
        "# a = tf.convert_to_tensor(a, np.float32)\n",
        "# b = np.arange(24).reshape(1,2,3,4)\n",
        "# b = tf.convert_to_tensor(b, np.float32)\n",
        "# product = tf.matmul(a, b, transpose_b=True)\n",
        "# print(product.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTfOzkdMbK2j"
      },
      "source": [
        "### Multi-Head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzN_spFEbS5K"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, nb_proj):\n",
        "        \"\"\"\n",
        "        :nb_proj: the number of projections for the multihead\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "\n",
        "    # this is the same as init but it happens when we USE the object for the\n",
        "    # first time, in init it was called when we CREATED the object\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we wanna make sure they are divisible\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        # we use 2 slashes to make it integer\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(self.d_model)\n",
        "        self.key_lin = layers.Dense(self.d_model)\n",
        "        self.value_lin = layers.Dense(self.d_model)\n",
        "        self.final_lin = layers.Dense(self.d_model)\n",
        "\n",
        "    def split_proj(self, inputs, batch_size):\n",
        "        \"\"\"\n",
        "        :inputs: [batch_size, seq_len(20), d_model(prev layer dim)]\n",
        "\n",
        "        :return:\n",
        "            dims = [batch_size, nb_proj, seq_len, d_proj]\n",
        "            nb_proj here is like channels in cnn\n",
        "            we basically split the d_model to nb_proj*d_proj so d_proj is\n",
        "            found by doing d_model/nb_proj\n",
        "        \"\"\"\n",
        "        new_shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        # here we will get: [batch_sz, seq_len, nb_proj, d_proj]\n",
        "\n",
        "        splited_inputs = tf.reshape(inputs, shape=new_shape)\n",
        "\n",
        "        # so we need to reshape it to: [batch_size, nb_proj, seq_len, d_proj]\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "\n",
        "        # now we split each of them to make projs\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        # each of the q,k,v are [batch_size, nb_proj, seq_len, d_proj]\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "        # now we will reverse the splits we did above: reshape + concat\n",
        "        attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "        # we have [batch_size, seq_len, nb_proj, d_proj] so now we concat 2, 3\n",
        "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N7jK4SvFZEb"
      },
      "source": [
        "## C - Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vEIb2g_5igO"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        \"\"\"\n",
        "        :FFN_units:\n",
        "            feed forward networks units: the number of units for the\n",
        "            feed forward which you can see in the encoder part of the\n",
        "            paper (right after the attention there is a feed forward...)\n",
        "        :nb_project:\n",
        "            the number of projections we have (8)\n",
        "        :dropout:\n",
        "            the dropout rate e.g. 0.3\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "    # we use this because we dont have many of the vars we want\n",
        "    # when we create the Encoder, so no we can get them when we use the\n",
        "    # function using 'build' instead\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we first build the object for the multi-head-attention\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model, activation=\"relu\")\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        \"\"\"\n",
        "        :mask: which we will apply in the multi-head attention\n",
        "        :training:\n",
        "            it is true/false which we use dropout while we train=true to stop\n",
        "            the model from overfiting but we dont use it when we are just\n",
        "            testing (aka. train=false)\n",
        "        \"\"\"\n",
        "        # if you look at the architecture you see that in the encoder\n",
        "        # all of the query/key/val are the same array which is the input we\n",
        "        # got from the previous layer\n",
        "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "\n",
        "        # dropout + normalization after the attention\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        # we do + inputs here because in the architecture they still concat the\n",
        "        # previous inputs to our resulted attention then we normalize it\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # now we do the dense in our FFN:\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABusZar2NKT6"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_encoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        # we put name=name here because the name is something that belongs\n",
        "        # to the layers class, so we tell it to use name=\"encoder\"\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_encoding_layers = nb_encoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(self.nb_encoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        # look at the paper's architecture while doing these\n",
        "        # embedding with maybe glove weights....\n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was because of what was writtent\n",
        "        # on the paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        # now we do dropout before all the encoding layers\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_encoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaf9z1oLXgjx"
      },
      "source": [
        "## D - Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXEa6VtkWP4b"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # MHA 1\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # MHA 2\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # FFN\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # check the architecture in the paper to see why we do these\n",
        "\n",
        "        # this is the 1# attention\n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # this is the 2# attention, this is ALOT different than before one\n",
        "        # pay attention to it's inputs\n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                enc_outputs,\n",
        "                                                enc_outputs,\n",
        "                                                mask_2)\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + inputs)\n",
        "\n",
        "        # the denses\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUUmK6hfoj7"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_decoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.nb_decoding_layers = nb_decoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(nb_decoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # look at the paper's architecture while doing these\n",
        "        # embedding with maybe glove weights....\n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was because of what was writtent\n",
        "        # on the paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        # now we do dropout before all the encoding layers\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_decoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                             enc_outputs,\n",
        "                                             mask_1,\n",
        "                                             mask_2,\n",
        "                                             training)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjQz4ZoHhm1v"
      },
      "source": [
        "## E - Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "begalnPMhXVT"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "\n",
        "        # initing the Objects\n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        # this is at the very end after you combined the enc & dec the output\n",
        "        # will of size vocab_dec\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "\n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # so this gives us element wise equal true/false with broadcasting on 0\n",
        "        # so we just want to see which words dont exist to give it true in all\n",
        "        # the batches\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        # so now we will return that mask we made but with 2 broadcasted new\n",
        "        # dimensions so it can match the input needed in attention\n",
        "        # (in the next cell I made and example to see it better)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        # sample of what it produces is in the cell below\n",
        "        # why do we do this? because when we predict the ith word we dont see\n",
        "        # words from\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        # combining the encoder and decoder and masks here\n",
        "\n",
        "        # creating the mask for encoder\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        # creating the mask for decoder\n",
        "\n",
        "        # mask #1 is for the first decoder attention which uses the\n",
        "        # output, output, output as q/k/v so we get max of the 2 masks for it\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
        "                                self.create_look_ahead_mask(dec_inputs))\n",
        "        # mask #2 is for the second decoder attention in which we use the\n",
        "        # output of the encoder as v/k so we need to do masking on the input\n",
        "        # so that later when doing q*k then *v we can get a correct output\n",
        "        # this is what the video said, but i belive making this None is alot                  # try making this none later\n",
        "        # more correct since we dont actually use the inputs and outputs but\n",
        "        # their already masked and processed outputs from previous attentions\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "\n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSRSyG4XwOp0"
      },
      "source": [
        "### testing masks to see their outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMj-AkOvn-Tl"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL6GXKykpln1",
        "outputId": "59a48ac0-4c5c-4bce-9c91-8cc04a69e23f"
      },
      "source": [
        "# sample: 1 batch of seq_len=8 ->[1,8] this will become [1,1,1,8] then\n",
        "# broadcasting will happen for later stages\n",
        "# mask=1 means we should delete this\n",
        "seq = tf.cast([[1, 2, 3, 0, 4, 0, 0, 0]], tf.int32)\n",
        "\n",
        "print('This padding masking as the name says shows which words exist: (later broadcasting will happen for each batch & nb_proj & d_proj')\n",
        "print(create_padding_mask(seq), end='\\n\\n')\n",
        "\n",
        "print(\"This look ahead masking as the name says shows that only for i>=j we need to keep them (=0's), so we should not see the feature indxes (i<j) (the 1's)\")\n",
        "print('Have in mind that mask=1 means we need to get rid of that, dont confuse it with mask=0')\n",
        "print(1 - tf.linalg.band_part(tf.ones((5, 5)), -1, 0), end='\\n\\n')\n",
        "\n",
        "print('Now applying both: (pay very close attention to this samples output, very important)')\n",
        "print(tf.maximum(create_padding_mask(seq),\n",
        "                 create_look_ahead_mask(seq)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This padding masking as the name says shows which words exist: (later broadcasting will happen for each batch & nb_proj & d_proj\n",
            "tf.Tensor([[[[0. 0. 0. 1. 0. 1. 1. 1.]]]], shape=(1, 1, 1, 8), dtype=float32)\n",
            "\n",
            "This look ahead masking as the name says shows that only for i>=j we need to keep them (=0's), so we should not see the feature indxes (i<j) (the 1's)\n",
            "Have in mind that mask=1 means we need to get rid of that, dont confuse it with mask=0\n",
            "tf.Tensor(\n",
            "[[0. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
            "\n",
            "Now applying both: (pay very close attention to this samples output, very important)\n",
            "tf.Tensor(\n",
            "[[[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]]]], shape=(1, 1, 8, 8), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzvSJakz0yUJ"
      },
      "source": [
        "# Step 4: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhfYzD5Oz7cg"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUemDV6BqMTs"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters:\n",
        "EPOCHS = 2\n",
        "D_MODEL = 128 # 512 takes more time but has a lot better results\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_DE,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woJ2_wR5kc7h"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OJChh-xscqd"
      },
      "source": [
        "# loss\n",
        "\n",
        "# 1) example of SparseCategoricalCrossentropy:\n",
        "# y_true = [1, 2]\n",
        "# y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
        "# 2) we made fromlogits=true. why? so it gives out numbers without softmax\n",
        "# applied to them\n",
        "# 3) we made reduction='none'. why? normally it would sum up all the losses form\n",
        "# all the batches to make it just 1 number but we dont want it, we want to get\n",
        "# rid of the padded losses ourselves then sum it up...\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "def loss_function(target, pred):\n",
        "    # so by using this mask we will get rid of all the losses that\n",
        "    # corrispond to 0 in our target (aka. y_true)\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0)) # [326, 4, 0] -> [1, 1, 0]\n",
        "    loss_ = loss_object(target, pred) # we got the loss numbers (not their softmax probabilities)\n",
        "\n",
        "    # make sure that both loss_ and mask have the same data type so we can mult them\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    # we make the computed losses for 0's 0\n",
        "    loss_ *= mask\n",
        "\n",
        "    # compute the mean loss and return\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# keeps track of losses during training\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "# keeps track of accs during training\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z_7kqLdkkFB"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL71RRe3kY3Y"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    # take notice that we used __call__ instead of call\n",
        "    # and that the parameter steps we dont give it, we get it\n",
        "    # from the tf.keras.optimizers.schedules.LearningRateSchedule itself\n",
        "    def __call__(self, step):\n",
        "        # read this part in paper and you'll understand arg1 & arg2 which they\n",
        "        # used in their custom learning rate\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TNz_bWzr1cI"
      },
      "source": [
        "## Checkpoints (delete your checkpoints if there are some unexpected errors when changing your data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9v_ZbpFpxZe"
      },
      "source": [
        "# making a checkpoint:\n",
        "checkpoint_path = \"./ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# lets check if we already have a checkpoint\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint Restored...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84_3qydkr-4N"
      },
      "source": [
        "## Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5Mfu-arnsN",
        "outputId": "91a903fd-b3eb-489d-a767-39a5c455d65f"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # iterate on each batch:\n",
        "    for (batch_index, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        # we take all the target minus the last word: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # why? because we are trying to predict the next word each time, so at the last step\n",
        "        # we are predicting <e> and we are done, so we wont need it as an input for our decoder\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        # we shift 1 to right because tokens are: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # when we want to do the predictions, we wont need to predict the <s>, we start with <s>\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "\n",
        "        # this will record everything that happens when we do predictions\n",
        "        with tf.GradientTape() as tape:\n",
        "            # the true is for training\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "        # now we get the gradients using this method using the tape\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        # now we apply the gradients according to our Adam optimizer\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        # now lets add our loss to the train loss object that keeps track of the loss\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        # now let's print our loss and acc from time to time.....\n",
        "        if batch_index % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch_index, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "\n",
        "    # at the end of each epoch we save a checkpoint\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}!\".format(epoch+1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.9532 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 5.9940 Accuracy 0.0020\n",
            "Epoch 1 Batch 100 Loss 5.9646 Accuracy 0.0283\n",
            "Epoch 1 Batch 150 Loss 5.8770 Accuracy 0.0389\n",
            "Epoch 1 Batch 200 Loss 5.8004 Accuracy 0.0488\n",
            "Epoch 1 Batch 250 Loss 5.7301 Accuracy 0.0565\n",
            "Epoch 1 Batch 300 Loss 5.6252 Accuracy 0.0627\n",
            "Epoch 1 Batch 350 Loss 5.5180 Accuracy 0.0670\n",
            "Epoch 1 Batch 400 Loss 5.4181 Accuracy 0.0705\n",
            "Epoch 1 Batch 450 Loss 5.3191 Accuracy 0.0731\n",
            "Epoch 1 Batch 500 Loss 5.2310 Accuracy 0.0761\n",
            "Epoch 1 Batch 550 Loss 5.1418 Accuracy 0.0795\n",
            "Epoch 1 Batch 600 Loss 5.0528 Accuracy 0.0829\n",
            "Epoch 1 Batch 650 Loss 4.9676 Accuracy 0.0863\n",
            "Epoch 1 Batch 700 Loss 4.8855 Accuracy 0.0899\n",
            "Epoch 1 Batch 750 Loss 4.8131 Accuracy 0.0936\n",
            "Epoch 1 Batch 800 Loss 4.7433 Accuracy 0.0975\n",
            "Epoch 1 Batch 850 Loss 4.6763 Accuracy 0.1014\n",
            "Epoch 1 Batch 900 Loss 4.6136 Accuracy 0.1053\n",
            "Epoch 1 Batch 950 Loss 4.5518 Accuracy 0.1091\n",
            "Epoch 1 Batch 1000 Loss 4.4971 Accuracy 0.1127\n",
            "Epoch 1 Batch 1050 Loss 4.4436 Accuracy 0.1162\n",
            "Epoch 1 Batch 1100 Loss 4.3918 Accuracy 0.1194\n",
            "Epoch 1 Batch 1150 Loss 4.3416 Accuracy 0.1224\n",
            "Epoch 1 Batch 1200 Loss 4.2943 Accuracy 0.1253\n",
            "Epoch 1 Batch 1250 Loss 4.2516 Accuracy 0.1280\n",
            "Epoch 1 Batch 1300 Loss 4.2104 Accuracy 0.1306\n",
            "Epoch 1 Batch 1350 Loss 4.1720 Accuracy 0.1331\n",
            "Epoch 1 Batch 1400 Loss 4.1351 Accuracy 0.1354\n",
            "Epoch 1 Batch 1450 Loss 4.0987 Accuracy 0.1377\n",
            "Epoch 1 Batch 1500 Loss 4.0642 Accuracy 0.1398\n",
            "Epoch 1 Batch 1550 Loss 4.0303 Accuracy 0.1419\n",
            "Epoch 1 Batch 1600 Loss 3.9983 Accuracy 0.1441\n",
            "Epoch 1 Batch 1650 Loss 3.9664 Accuracy 0.1461\n",
            "Epoch 1 Batch 1700 Loss 3.9378 Accuracy 0.1481\n",
            "Epoch 1 Batch 1750 Loss 3.9080 Accuracy 0.1500\n",
            "Epoch 1 Batch 1800 Loss 3.8806 Accuracy 0.1519\n",
            "Epoch 1 Batch 1850 Loss 3.8517 Accuracy 0.1536\n",
            "Epoch 1 Batch 1900 Loss 3.8253 Accuracy 0.1553\n",
            "Epoch 1 Batch 1950 Loss 3.7987 Accuracy 0.1571\n",
            "Epoch 1 Batch 2000 Loss 3.7732 Accuracy 0.1587\n",
            "Epoch 1 Batch 2050 Loss 3.7480 Accuracy 0.1603\n",
            "Epoch 1 Batch 2100 Loss 3.7241 Accuracy 0.1618\n",
            "Epoch 1 Batch 2150 Loss 3.7009 Accuracy 0.1634\n",
            "Epoch 1 Batch 2200 Loss 3.6773 Accuracy 0.1650\n",
            "Epoch 1 Batch 2250 Loss 3.6555 Accuracy 0.1664\n",
            "Epoch 1 Batch 2300 Loss 3.6330 Accuracy 0.1679\n",
            "Epoch 1 Batch 2350 Loss 3.6123 Accuracy 0.1692\n",
            "Epoch 1 Batch 2400 Loss 3.5910 Accuracy 0.1706\n",
            "Epoch 1 Batch 2450 Loss 3.5713 Accuracy 0.1721\n",
            "Epoch 1 Batch 2500 Loss 3.5507 Accuracy 0.1734\n",
            "Epoch 1 Batch 2550 Loss 3.5304 Accuracy 0.1749\n",
            "Epoch 1 Batch 2600 Loss 3.5099 Accuracy 0.1763\n",
            "Epoch 1 Batch 2650 Loss 3.4914 Accuracy 0.1778\n",
            "Epoch 1 Batch 2700 Loss 3.4735 Accuracy 0.1792\n",
            "Epoch 1 Batch 2750 Loss 3.4557 Accuracy 0.1807\n",
            "Epoch 1 Batch 2800 Loss 3.4384 Accuracy 0.1821\n",
            "Epoch 1 Batch 2850 Loss 3.4215 Accuracy 0.1835\n",
            "Epoch 1 Batch 2900 Loss 3.4053 Accuracy 0.1849\n",
            "Epoch 1 Batch 2950 Loss 3.3890 Accuracy 0.1863\n",
            "Epoch 1 Batch 3000 Loss 3.3731 Accuracy 0.1877\n",
            "Epoch 1 Batch 3050 Loss 3.3576 Accuracy 0.1891\n",
            "Epoch 1 Batch 3100 Loss 3.3430 Accuracy 0.1903\n",
            "Epoch 1 Batch 3150 Loss 3.3280 Accuracy 0.1917\n",
            "Epoch 1 Batch 3200 Loss 3.3144 Accuracy 0.1930\n",
            "Epoch 1 Batch 3250 Loss 3.3009 Accuracy 0.1943\n",
            "Epoch 1 Batch 3300 Loss 3.2874 Accuracy 0.1956\n",
            "Epoch 1 Batch 3350 Loss 3.2740 Accuracy 0.1970\n",
            "Epoch 1 Batch 3400 Loss 3.2602 Accuracy 0.1983\n",
            "Epoch 1 Batch 3450 Loss 3.2475 Accuracy 0.1996\n",
            "Epoch 1 Batch 3500 Loss 3.2341 Accuracy 0.2009\n",
            "Epoch 1 Batch 3550 Loss 3.2212 Accuracy 0.2021\n",
            "Epoch 1 Batch 3600 Loss 3.2081 Accuracy 0.2034\n",
            "Epoch 1 Batch 3650 Loss 3.1950 Accuracy 0.2046\n",
            "Epoch 1 Batch 3700 Loss 3.1827 Accuracy 0.2058\n",
            "Epoch 1 Batch 3750 Loss 3.1706 Accuracy 0.2070\n",
            "Epoch 1 Batch 3800 Loss 3.1590 Accuracy 0.2083\n",
            "Epoch 1 Batch 3850 Loss 3.1469 Accuracy 0.2095\n",
            "Epoch 1 Batch 3900 Loss 3.1352 Accuracy 0.2107\n",
            "Epoch 1 Batch 3950 Loss 3.1243 Accuracy 0.2119\n",
            "Epoch 1 Batch 4000 Loss 3.1125 Accuracy 0.2131\n",
            "Epoch 1 Batch 4050 Loss 3.1012 Accuracy 0.2142\n",
            "Epoch 1 Batch 4100 Loss 3.0903 Accuracy 0.2154\n",
            "Epoch 1 Batch 4150 Loss 3.0790 Accuracy 0.2166\n",
            "Epoch 1 Batch 4200 Loss 3.0680 Accuracy 0.2178\n",
            "Epoch 1 Batch 4250 Loss 3.0574 Accuracy 0.2190\n",
            "Epoch 1 Batch 4300 Loss 3.0469 Accuracy 0.2203\n",
            "Epoch 1 Batch 4350 Loss 3.0359 Accuracy 0.2215\n",
            "Epoch 1 Batch 4400 Loss 3.0255 Accuracy 0.2226\n",
            "Epoch 1 Batch 4450 Loss 3.0157 Accuracy 0.2236\n",
            "Epoch 1 Batch 4500 Loss 3.0061 Accuracy 0.2246\n",
            "Epoch 1 Batch 4550 Loss 2.9970 Accuracy 0.2256\n",
            "Epoch 1 Batch 4600 Loss 2.9886 Accuracy 0.2265\n",
            "Epoch 1 Batch 4650 Loss 2.9799 Accuracy 0.2273\n",
            "Epoch 1 Batch 4700 Loss 2.9713 Accuracy 0.2281\n",
            "Epoch 1 Batch 4750 Loss 2.9630 Accuracy 0.2289\n",
            "Epoch 1 Batch 4800 Loss 2.9550 Accuracy 0.2296\n",
            "Epoch 1 Batch 4850 Loss 2.9470 Accuracy 0.2304\n",
            "Epoch 1 Batch 4900 Loss 2.9384 Accuracy 0.2311\n",
            "Epoch 1 Batch 4950 Loss 2.9302 Accuracy 0.2319\n",
            "Epoch 1 Batch 5000 Loss 2.9223 Accuracy 0.2327\n",
            "Epoch 1 Batch 5050 Loss 2.9144 Accuracy 0.2334\n",
            "Epoch 1 Batch 5100 Loss 2.9066 Accuracy 0.2341\n",
            "Epoch 1 Batch 5150 Loss 2.8992 Accuracy 0.2348\n",
            "Epoch 1 Batch 5200 Loss 2.8915 Accuracy 0.2356\n",
            "Epoch 1 Batch 5250 Loss 2.8835 Accuracy 0.2363\n",
            "Epoch 1 Batch 5300 Loss 2.8761 Accuracy 0.2371\n",
            "Epoch 1 Batch 5350 Loss 2.8688 Accuracy 0.2378\n",
            "Epoch 1 Batch 5400 Loss 2.8615 Accuracy 0.2384\n",
            "Epoch 1 Batch 5450 Loss 2.8541 Accuracy 0.2392\n",
            "Epoch 1 Batch 5500 Loss 2.8468 Accuracy 0.2398\n",
            "Epoch 1 Batch 5550 Loss 2.8396 Accuracy 0.2405\n",
            "Epoch 1 Batch 5600 Loss 2.8325 Accuracy 0.2411\n",
            "Epoch 1 Batch 5650 Loss 2.8251 Accuracy 0.2418\n",
            "Epoch 1 Batch 5700 Loss 2.8177 Accuracy 0.2424\n",
            "Epoch 1 Batch 5750 Loss 2.8105 Accuracy 0.2430\n",
            "Epoch 1 Batch 5800 Loss 2.8037 Accuracy 0.2436\n",
            "Epoch 1 Batch 5850 Loss 2.7962 Accuracy 0.2442\n",
            "Epoch 1 Batch 5900 Loss 2.7893 Accuracy 0.2448\n",
            "Epoch 1 Batch 5950 Loss 2.7823 Accuracy 0.2454\n",
            "Epoch 1 Batch 6000 Loss 2.7752 Accuracy 0.2460\n",
            "Epoch 1 Batch 6050 Loss 2.7680 Accuracy 0.2466\n",
            "Epoch 1 Batch 6100 Loss 2.7610 Accuracy 0.2472\n",
            "Saved checkpoint for epoch 1!\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.8451 Accuracy 0.3141\n",
            "Epoch 2 Batch 50 Loss 1.9950 Accuracy 0.3183\n",
            "Epoch 2 Batch 100 Loss 1.9824 Accuracy 0.3196\n",
            "Epoch 2 Batch 150 Loss 1.9810 Accuracy 0.3182\n",
            "Epoch 2 Batch 200 Loss 1.9808 Accuracy 0.3194\n",
            "Epoch 2 Batch 250 Loss 1.9689 Accuracy 0.3202\n",
            "Epoch 2 Batch 300 Loss 1.9639 Accuracy 0.3205\n",
            "Epoch 2 Batch 350 Loss 1.9608 Accuracy 0.3206\n",
            "Epoch 2 Batch 400 Loss 1.9511 Accuracy 0.3218\n",
            "Epoch 2 Batch 450 Loss 1.9465 Accuracy 0.3229\n",
            "Epoch 2 Batch 500 Loss 1.9409 Accuracy 0.3236\n",
            "Epoch 2 Batch 550 Loss 1.9335 Accuracy 0.3239\n",
            "Epoch 2 Batch 600 Loss 1.9279 Accuracy 0.3241\n",
            "Epoch 2 Batch 650 Loss 1.9219 Accuracy 0.3247\n",
            "Epoch 2 Batch 700 Loss 1.9205 Accuracy 0.3250\n",
            "Epoch 2 Batch 750 Loss 1.9214 Accuracy 0.3252\n",
            "Epoch 2 Batch 800 Loss 1.9205 Accuracy 0.3254\n",
            "Epoch 2 Batch 850 Loss 1.9199 Accuracy 0.3256\n",
            "Epoch 2 Batch 900 Loss 1.9177 Accuracy 0.3258\n",
            "Epoch 2 Batch 950 Loss 1.9161 Accuracy 0.3260\n",
            "Epoch 2 Batch 1000 Loss 1.9127 Accuracy 0.3263\n",
            "Epoch 2 Batch 1050 Loss 1.9125 Accuracy 0.3266\n",
            "Epoch 2 Batch 1100 Loss 1.9085 Accuracy 0.3270\n",
            "Epoch 2 Batch 1150 Loss 1.9049 Accuracy 0.3272\n",
            "Epoch 2 Batch 1200 Loss 1.9025 Accuracy 0.3274\n",
            "Epoch 2 Batch 1250 Loss 1.8992 Accuracy 0.3275\n",
            "Epoch 2 Batch 1300 Loss 1.8948 Accuracy 0.3276\n",
            "Epoch 2 Batch 1350 Loss 1.8923 Accuracy 0.3280\n",
            "Epoch 2 Batch 1400 Loss 1.8897 Accuracy 0.3284\n",
            "Epoch 2 Batch 1450 Loss 1.8876 Accuracy 0.3286\n",
            "Epoch 2 Batch 1500 Loss 1.8843 Accuracy 0.3289\n",
            "Epoch 2 Batch 1550 Loss 1.8805 Accuracy 0.3293\n",
            "Epoch 2 Batch 1600 Loss 1.8783 Accuracy 0.3297\n",
            "Epoch 2 Batch 1650 Loss 1.8749 Accuracy 0.3300\n",
            "Epoch 2 Batch 1700 Loss 1.8716 Accuracy 0.3303\n",
            "Epoch 2 Batch 1750 Loss 1.8684 Accuracy 0.3306\n",
            "Epoch 2 Batch 1800 Loss 1.8643 Accuracy 0.3309\n",
            "Epoch 2 Batch 1850 Loss 1.8608 Accuracy 0.3311\n",
            "Epoch 2 Batch 1900 Loss 1.8567 Accuracy 0.3314\n",
            "Epoch 2 Batch 1950 Loss 1.8523 Accuracy 0.3315\n",
            "Epoch 2 Batch 2000 Loss 1.8489 Accuracy 0.3318\n",
            "Epoch 2 Batch 2050 Loss 1.8453 Accuracy 0.3320\n",
            "Epoch 2 Batch 2100 Loss 1.8415 Accuracy 0.3322\n",
            "Epoch 2 Batch 2150 Loss 1.8367 Accuracy 0.3324\n",
            "Epoch 2 Batch 2200 Loss 1.8325 Accuracy 0.3327\n",
            "Epoch 2 Batch 2250 Loss 1.8289 Accuracy 0.3330\n",
            "Epoch 2 Batch 2300 Loss 1.8248 Accuracy 0.3332\n",
            "Epoch 2 Batch 2350 Loss 1.8217 Accuracy 0.3336\n",
            "Epoch 2 Batch 2400 Loss 1.8179 Accuracy 0.3339\n",
            "Epoch 2 Batch 2450 Loss 1.8133 Accuracy 0.3343\n",
            "Epoch 2 Batch 2500 Loss 1.8092 Accuracy 0.3346\n",
            "Epoch 2 Batch 2550 Loss 1.8046 Accuracy 0.3349\n",
            "Epoch 2 Batch 2600 Loss 1.8007 Accuracy 0.3354\n",
            "Epoch 2 Batch 2650 Loss 1.7968 Accuracy 0.3358\n",
            "Epoch 2 Batch 2700 Loss 1.7934 Accuracy 0.3365\n",
            "Epoch 2 Batch 2750 Loss 1.7903 Accuracy 0.3370\n",
            "Epoch 2 Batch 2800 Loss 1.7864 Accuracy 0.3375\n",
            "Epoch 2 Batch 2850 Loss 1.7829 Accuracy 0.3380\n",
            "Epoch 2 Batch 2900 Loss 1.7794 Accuracy 0.3385\n",
            "Epoch 2 Batch 2950 Loss 1.7769 Accuracy 0.3391\n",
            "Epoch 2 Batch 3000 Loss 1.7741 Accuracy 0.3396\n",
            "Epoch 2 Batch 3050 Loss 1.7713 Accuracy 0.3402\n",
            "Epoch 2 Batch 3100 Loss 1.7685 Accuracy 0.3407\n",
            "Epoch 2 Batch 3150 Loss 1.7654 Accuracy 0.3413\n",
            "Epoch 2 Batch 3200 Loss 1.7625 Accuracy 0.3418\n",
            "Epoch 2 Batch 3250 Loss 1.7596 Accuracy 0.3424\n",
            "Epoch 2 Batch 3300 Loss 1.7569 Accuracy 0.3430\n",
            "Epoch 2 Batch 3350 Loss 1.7542 Accuracy 0.3436\n",
            "Epoch 2 Batch 3400 Loss 1.7513 Accuracy 0.3443\n",
            "Epoch 2 Batch 3450 Loss 1.7490 Accuracy 0.3449\n",
            "Epoch 2 Batch 3500 Loss 1.7460 Accuracy 0.3455\n",
            "Epoch 2 Batch 3550 Loss 1.7427 Accuracy 0.3460\n",
            "Epoch 2 Batch 3600 Loss 1.7394 Accuracy 0.3466\n",
            "Epoch 2 Batch 3650 Loss 1.7361 Accuracy 0.3472\n",
            "Epoch 2 Batch 3700 Loss 1.7331 Accuracy 0.3478\n",
            "Epoch 2 Batch 3750 Loss 1.7302 Accuracy 0.3485\n",
            "Epoch 2 Batch 3800 Loss 1.7277 Accuracy 0.3491\n",
            "Epoch 2 Batch 3850 Loss 1.7243 Accuracy 0.3498\n",
            "Epoch 2 Batch 3900 Loss 1.7215 Accuracy 0.3504\n",
            "Epoch 2 Batch 3950 Loss 1.7184 Accuracy 0.3510\n",
            "Epoch 2 Batch 4000 Loss 1.7153 Accuracy 0.3517\n",
            "Epoch 2 Batch 4050 Loss 1.7122 Accuracy 0.3524\n",
            "Epoch 2 Batch 4100 Loss 1.7096 Accuracy 0.3530\n",
            "Epoch 2 Batch 4150 Loss 1.7071 Accuracy 0.3536\n",
            "Epoch 2 Batch 4200 Loss 1.7042 Accuracy 0.3542\n",
            "Epoch 2 Batch 4250 Loss 1.7014 Accuracy 0.3549\n",
            "Epoch 2 Batch 4300 Loss 1.6986 Accuracy 0.3556\n",
            "Epoch 2 Batch 4350 Loss 1.6961 Accuracy 0.3562\n",
            "Epoch 2 Batch 4400 Loss 1.6939 Accuracy 0.3568\n",
            "Epoch 2 Batch 4450 Loss 1.6921 Accuracy 0.3573\n",
            "Epoch 2 Batch 4500 Loss 1.6909 Accuracy 0.3577\n",
            "Epoch 2 Batch 4550 Loss 1.6899 Accuracy 0.3580\n",
            "Epoch 2 Batch 4600 Loss 1.6891 Accuracy 0.3581\n",
            "Epoch 2 Batch 4650 Loss 1.6886 Accuracy 0.3584\n",
            "Epoch 2 Batch 4700 Loss 1.6881 Accuracy 0.3585\n",
            "Epoch 2 Batch 4750 Loss 1.6877 Accuracy 0.3587\n",
            "Epoch 2 Batch 4800 Loss 1.6871 Accuracy 0.3588\n",
            "Epoch 2 Batch 4850 Loss 1.6870 Accuracy 0.3589\n",
            "Epoch 2 Batch 4900 Loss 1.6864 Accuracy 0.3590\n",
            "Epoch 2 Batch 4950 Loss 1.6862 Accuracy 0.3591\n",
            "Epoch 2 Batch 5000 Loss 1.6856 Accuracy 0.3593\n",
            "Epoch 2 Batch 5050 Loss 1.6851 Accuracy 0.3593\n",
            "Epoch 2 Batch 5100 Loss 1.6845 Accuracy 0.3595\n",
            "Epoch 2 Batch 5150 Loss 1.6840 Accuracy 0.3596\n",
            "Epoch 2 Batch 5200 Loss 1.6836 Accuracy 0.3597\n",
            "Epoch 2 Batch 5250 Loss 1.6832 Accuracy 0.3598\n",
            "Epoch 2 Batch 5300 Loss 1.6829 Accuracy 0.3599\n",
            "Epoch 2 Batch 5350 Loss 1.6827 Accuracy 0.3601\n",
            "Epoch 2 Batch 5400 Loss 1.6825 Accuracy 0.3602\n",
            "Epoch 2 Batch 5450 Loss 1.6822 Accuracy 0.3603\n",
            "Epoch 2 Batch 5500 Loss 1.6816 Accuracy 0.3603\n",
            "Epoch 2 Batch 5550 Loss 1.6810 Accuracy 0.3604\n",
            "Epoch 2 Batch 5600 Loss 1.6801 Accuracy 0.3605\n",
            "Epoch 2 Batch 5650 Loss 1.6797 Accuracy 0.3605\n",
            "Epoch 2 Batch 5700 Loss 1.6791 Accuracy 0.3605\n",
            "Epoch 2 Batch 5750 Loss 1.6781 Accuracy 0.3606\n",
            "Epoch 2 Batch 5800 Loss 1.6775 Accuracy 0.3606\n",
            "Epoch 2 Batch 5850 Loss 1.6770 Accuracy 0.3607\n",
            "Epoch 2 Batch 5900 Loss 1.6764 Accuracy 0.3607\n",
            "Epoch 2 Batch 5950 Loss 1.6757 Accuracy 0.3607\n",
            "Epoch 2 Batch 6000 Loss 1.6750 Accuracy 0.3607\n",
            "Epoch 2 Batch 6050 Loss 1.6742 Accuracy 0.3607\n",
            "Epoch 2 Batch 6100 Loss 1.6733 Accuracy 0.3608\n",
            "Saved checkpoint for epoch 2!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DboKBdVg48mc"
      },
      "source": [
        "# Step 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LceVAvXNv7pV"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    # turn the sentence to the tokenizer_encoded format [hi, bye] -> [241, 6]\n",
        "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    # expand dim on axis=0 to simulate the batch dimmension\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    # let's make the ouput which starts with <s> and add that axis=0 for batch=0\n",
        "    output = tf.expand_dims([VOCAB_SIZE_DE-2], axis=0)\n",
        "\n",
        "    # the loop to predict the next word of output each time and output += it\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        # we put false because we are not training so no dropout\n",
        "        # predictions = [btch_sz=1, seq_len(output_so_far), vocav_sz_de(the\n",
        "        # softmax values of each word, the higher the number the higher the\n",
        "        # probability for that word)]\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        # we want to take the last word of this prediction\n",
        "        prediction = predictions[:, -1:, :]\n",
        "        # we do argmax to get the index of the most probable next word\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        # we reached the end of the sentence\n",
        "        if predicted_id == VOCAB_SIZE_DE-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "\n",
        "        # now we know add the new prediction to the last of the output\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    #even if we didn't reach the end of the sentence we can't continue\n",
        "    return tf.squeeze(output, axis=0)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXL5DaPPhKg"
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    # get rid of <s> and <e> if they exist\n",
        "    output = [i for i in output if i < VOCAB_SIZE_DE-2]\n",
        "    # decode indexes to words e.g. [241, 6] -> [hi, bye]\n",
        "    predicted_sentence = tokenizer_de.decode(output)\n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGOu9PO7Q797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7191fe1-94e8-486a-8d32-45fa07f23e52"
      },
      "source": [
        "translate(\"This is a great day!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: This is a great day!\n",
            "Predicted translation: Das ist ein gro√üer Tag!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNIHveQF7Bib"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}